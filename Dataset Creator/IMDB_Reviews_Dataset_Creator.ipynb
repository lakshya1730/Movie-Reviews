{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqE-AnbKF3bq"
   },
   "source": [
    "Installing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "WxdTCIjHF7Yc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imdbpy\n",
      "  Downloading IMDbPY-2020.9.25-py3-none-any.whl (304 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from imdbpy) (4.5.0)\n",
      "Requirement already satisfied: SQLAlchemy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from imdbpy) (1.3.13)\n",
      "Installing collected packages: imdbpy\n",
      "Successfully installed imdbpy-2020.9.25\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.4.1-py2.py3-none-any.whl (239 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scrapy) (19.1.0)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.2.0-cp37-cp37m-win_amd64.whl (196 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0-cp37-cp37m-win_amd64.whl (3.1 MB)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scrapy) (2.8)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scrapy) (4.5.0)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyOpenSSL>=16.2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (45.2.0.post20200210)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-20.0.1-py2.py3-none-any.whl (48 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Building wheels for collected packages: PyDispatcher, protego\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=12552 sha256=b00e246b736b14793f8fc5f1bd7eeb0572362580db8a4ca437da3cf5699b6f5a\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\dc\\d0\\bf\\0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "  Building wheel for protego (setup.py): started\n",
      "  Building wheel for protego (setup.py): finished with status 'done'\n",
      "  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7769 sha256=a8999991e39fe8f50c139a9187f48111222dbeb9b006028f7b66a41cdf181286\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\ca\\44\\01\\3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "Successfully built PyDispatcher protego\n",
      "Installing collected packages: pyasn1, pyasn1-modules, service-identity, zope.interface, hyperlink, PyHamcrest, constantly, Automat, incremental, Twisted, PyDispatcher, queuelib, protego, w3lib, jmespath, itemadapter, cssselect, parsel, itemloaders, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-20.0.1 incremental-17.5.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.5.0 scrapy-2.4.1 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.2.0\n",
      "Requirement already satisfied: bs4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from bs4) (4.8.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from selenium) (1.25.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.5)\n"
     ]
    }
   ],
   "source": [
    "#package to help us get the movie names for our dataset\n",
    "!pip install imdbpy\n",
    "!pip install scrapy\n",
    "!pip install bs4 selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "griMeL37HGmU"
   },
   "source": [
    "importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C8KVdmMwHJbW"
   },
   "outputs": [],
   "source": [
    "import imdb \n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from selenium import webdriver      \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vWTTeHHFM1J"
   },
   "source": [
    "Getting a list of top 250 movies from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G-xonzfTGJBC"
   },
   "outputs": [],
   "source": [
    "def imdb_movie_list():\n",
    "    ia = imdb.IMDb()\n",
    "    search = ia.get_top250_movies()\n",
    "    a=[]\n",
    "    for title in range (len(search)):\n",
    "        a.append(search[title]['title'])\n",
    "  \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5Vk4gtIIzKZ"
   },
   "source": [
    "Creating a function to extract exact movie URL once we reach the search window\n",
    "\n",
    "\n",
    "* This URL function will help us get the unique movie title code corresponding to each movie within imdb servers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZySGmoSiI6el"
   },
   "outputs": [],
   "source": [
    "def movie_url_funct(movie_url):\n",
    "    response = get(movie_url)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result_container = html_soup.find_all('table', class_ = 'findList')\n",
    "    top_result=result_container[0]\n",
    "    top_result=top_result.find('td', class_ = 'result_text')\n",
    "    return ('https://www.imdb.com'+top_result.a.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYHUmYNcV_oO"
   },
   "source": [
    "Once we extracted the movie id and urls from the function above we can use this function to jump to all user reviews page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xgmb5qN6V36e"
   },
   "outputs": [],
   "source": [
    "version_regex = re.compile(r\"\\/title\\/[a-zA-Z0-9]\\w+\\/reviews\")# regex to extract all user reviews href link\n",
    "\n",
    "def all_reviews_url_funct(true_movie_url):\n",
    "    response=get(true_movie_url)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    review_result_container = html_soup.findAll('div',class_='user-comments')# this class has the user review tags\n",
    "    x=review_result_container[0]\n",
    "    all_reviews_url=x.findAll('a') # to find all the <a> sections which contains the href parameters\n",
    "\n",
    "    for x in all_reviews_url:\n",
    "        a=x.get('href')\n",
    "        found=False\n",
    "        for match in version_regex.finditer(a):\n",
    "            return ('https://www.imdb.com/'+ match.group())\n",
    "            found=True\n",
    "        if not found:\n",
    "              pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VidbjEhbYYwG"
   },
   "source": [
    "Once you have the all user review page, now we can start extracting the reviews from this page :D\n",
    "Few things to keep in mind are:\n",
    "\n",
    "\n",
    "\n",
    "*   We will use a selenium based option combined with beautiful soup to help click on the load more button and extract all the reviews which we want.\n",
    "*   we need to extract keeping in mind that the api calls are not too frequent, we'll implement some logic that embeds sleep timer in the code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK37xMS5s6kP"
   },
   "source": [
    "**Intializing the selenium browser which will extract everything on our behalf**\n",
    "\n",
    "to download the chromedriver please visit the link and download the version which corresponds to your current google chrome browser:\n",
    "https://sites.google.com/a/chromium.org/chromedriver/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zn1D_wvhs3Bs"
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(r\"E:\\chromedriver.exe\", options=options,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rzXC2SUuZoK"
   },
   "source": [
    "Defining the function that keeps clicking the load more button till the entire review page has loaded and then fetches the page source of the final page, which in turn gives us all the user reviews corresponding to the entered review URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "p37DSeL1uYSA"
   },
   "outputs": [],
   "source": [
    " \n",
    "def pagesource_final(url_name,load_more_button_xpath):\n",
    "    driver.get(url_name)\n",
    "    time.sleep(2)\n",
    "    while True:        \n",
    "        try:\n",
    "            loadMoreButton = driver.find_element_by_xpath(load_more_button_xpath)\n",
    "            time.sleep(2)\n",
    "            driver.execute_script(\"window.scrollTo(0, window.scrollY + 100)\") #scrolls the page in case the button is no available yet\n",
    "            driver.implicitly_wait(3)\n",
    "            ActionChains(driver).move_to_element(loadMoreButton).click(loadMoreButton).perform()#clicks the load more button\n",
    "            time.sleep(3)\n",
    "            page_source=driver.page_source\n",
    "    #         driver.get(YOUTUBER_HOME_PAGE_URL)   \n",
    "        except Exception as e:\n",
    "            page_source=driver.page_source\n",
    "    #             print(e)\n",
    "            break\n",
    "    return page_source\n",
    "    print (\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eozcat2tvPyO"
   },
   "source": [
    "Creating the function that will finally create our Reviews dataset based on the page source extracted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "aaV0D53ovXHl"
   },
   "outputs": [],
   "source": [
    "def imdb_review_dataset_creator(page_source):\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    #Get the title of the movie\n",
    "    all = soup.find(id=\"main\")\n",
    "    parent = all.find(class_ =\"parent\")\n",
    "    name = parent.find(itemprop = \"name\")\n",
    "    # url = name.find(itemprop = 'url')\n",
    "    # film_title = url.get_text()\n",
    "\n",
    "    #get the movie name\n",
    "    movie_name=parent.a.text.replace(\"\\n\", \"\")\n",
    "\n",
    "    #get the movie year\n",
    "    year=parent.span.text.replace(\" \", \"\").replace(\"\\n\",\"\").replace(\"(\",\"\").replace(\")\",\"\")\n",
    "\n",
    "    #Get the title of the review\n",
    "    title_rev = all.select(\".title\")\n",
    "    title = [t.get_text().replace(\"\\n\", \"\") for t in title_rev]\n",
    "\n",
    "\n",
    "    #Get the review\n",
    "    review_rev = all.select(\".content .text\")\n",
    "    review = [r.get_text() for r in review_rev]\n",
    "    \n",
    "    table_review = pd.DataFrame({\n",
    "    \"Movie\" : movie_name,\n",
    "    \"Year Released\": year,\n",
    "    \"Review Title\" : title,\n",
    "    \"Review\" : review\n",
    "    })\n",
    "    \n",
    "    return  table_review\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ5NQBBNwDzf"
   },
   "source": [
    "Calling all the functions to create our much awaited dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4qaGtFc11TW"
   },
   "source": [
    "This functions gets the list of top 250 movies from IMDB\n",
    "you can basically have any movie name of your choice within this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H0Vdb7Ae19qp"
   },
   "outputs": [],
   "source": [
    "movie_list=imdb_movie_list()\n",
    "movie_list = [x.replace(' ','+') for x in movie_list] #adding a + instead of whitespace since that's how IMDB parses URL\n",
    "search_url_list=['https://www.imdb.com/find?q='+x for x in movie_list]# creating IMDB URLs for each movie list we fethced above\n",
    "search_url_list=[x+'&s=tt&ref_=fn_tt' for x in search_url_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCFSq4ou2uy_"
   },
   "source": [
    "calling the function to get unique movie title for each movie keyword we send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QfMwP9Fq2x3_"
   },
   "outputs": [],
   "source": [
    "movie_title_url=[]\n",
    "for movie_url in search_url_list:\n",
    "    movie_title_url.append(movie_url_funct(movie_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdvp4WzZ3OvI"
   },
   "source": [
    "Once we have the unique title url for each movie we extract the URL that takes us to user review page for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ydv-xgXp3XiT"
   },
   "outputs": [],
   "source": [
    "user_review_url=[]\n",
    "for unique_movie_url in movie_title_url:\n",
    "    user_review_url.append(all_reviews_url_funct(unique_movie_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6sDKnHH4gl-"
   },
   "source": [
    "Calling the function that clicks on load more button and finally stores the page source in a variable, we then use that final page source to extract the info we need and append it to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "H2U0EtFSwY6U"
   },
   "outputs": [],
   "source": [
    "page_source=''\n",
    "page_source_list=[]\n",
    "load_more_button_xpath='//*[@id=\"load-more-trigger\"]'\n",
    "\n",
    "for review_url in user_review_url:\n",
    "    page_source_list.append(pagesource_final(review_url,load_more_button_xpath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKskvTexvoPQ"
   },
   "source": [
    "**Declaring the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qx-Vifvgvt8i"
   },
   "outputs": [],
   "source": [
    "#Make it into dataframe\n",
    "IMDB_dataset_creator = pd.DataFrame(columns={\n",
    "    \"Movie\" ,\n",
    "    \"Year Released\",\n",
    "    \"Review Title\",\n",
    "    \"Review\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "fYJWUNV_yEQk"
   },
   "outputs": [],
   "source": [
    "for page_source in page_source_list:\n",
    "    a=imdb_review_dataset_creator(page_source)\n",
    "    IMDB_dataset_creator=IMDB_dataset_creator.append(a,ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "IMDB Reviews Dataset Creator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
